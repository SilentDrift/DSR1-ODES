# DeepSeekâ€‘ODEâ€‘Tutor â€“ **Production-Ready Setup**

A complete, productionâ€‘ready skeleton that you can deploy on a **Windows GPU server** and run either as a commandâ€‘line tutor or a REST API.

---

## ğŸš€ **Quick Start**

1. **Clone and setup**
```powershell
git clone <your-repo-url> DeepSeek-ODE-Tutor
cd DeepSeek-ODE-Tutor
scripts\setup.ps1
```

2. **Test your setup**
```powershell
python test_setup.py
```

3. **Run interactive tutor**
```powershell
scripts\run_cli.ps1
```

4. **Or run as API server**
```powershell
scripts\run_api.ps1
```

---

## ğŸ“ **Project Structure**

```
DeepSeek-ODE-Tutor/
â”œâ”€â”€ README.md                 â† Setup & usage instructions
â”œâ”€â”€ environment.yml           â† Conda environment (GPU-optimized)
â”œâ”€â”€ requirements.txt          â† Pip fallback dependencies
â”œâ”€â”€ test_setup.py             â† Verify installation
â”œâ”€â”€ download_weights.py       â† Download model weights (~3.6 GB)
â”œâ”€â”€ .gitignore               
â”œâ”€â”€ config/
â”‚   â””â”€â”€ settings.yaml         â† Model & generation parameters
â”œâ”€â”€ models/                   â† Model weights (auto-created)
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ chatbot.py            â† Core inference engine
â”‚   â”œâ”€â”€ cli.py                â† Interactive console
â”‚   â””â”€â”€ api.py                â† FastAPI web service
â””â”€â”€ scripts/
    â”œâ”€â”€ setup.ps1             â† One-click setup
    â”œâ”€â”€ run_cli.ps1           â† Launch CLI
    â””â”€â”€ run_api.ps1           â† Launch API server
```

---

## âš™ï¸ **Prerequisites**

### Required Software
- **Git for Windows** 
- **Miniconda â‰¥ 23.11** ([Download](https://docs.conda.io/en/latest/miniconda.html))
- **NVIDIA Driver + CUDA 12.1+** (for GPU acceleration)

### Hardware Requirements
- **GPU**: NVIDIA GPU with â‰¥4 GB VRAM (recommended)
- **RAM**: â‰¥8 GB system RAM  
- **Storage**: â‰¥5 GB free space for model weights

---

## ğŸ”§ **Detailed Setup**

### 1. Clone Repository
```powershell
git clone <your-repo-url> DeepSeek-ODE-Tutor
cd DeepSeek-ODE-Tutor
```

### 2. Run Automated Setup
```powershell
scripts\setup.ps1
```

This script will:
- Create `deepseek-ode` conda environment
- Install PyTorch with CUDA support
- Install all Python dependencies  
- Download DeepSeek-R1-1.5B model weights (~3.6 GB)

### 3. Verify Installation
```powershell
python test_setup.py
```

Expected output:
```
ğŸ§ª DeepSeek ODE Tutor Setup Test

ğŸ”§ Testing imports...
  âœ… torch
  âœ… transformers
  âœ… accelerate
  ...
âœ… All packages available

ğŸ”§ Testing CUDA...
  âœ… CUDA available: NVIDIA GeForce RTX 4090
  ğŸ“Š GPU Memory: 24.0 GB

ğŸ“Š Test Results: 5/5 passed
ğŸ‰ All tests passed! Your setup looks good.
```

---

## ğŸ–¥ï¸ **Usage**

### Interactive CLI Mode
```powershell
scripts\run_cli.ps1
```

Example session:
```
âœ… DeepSeek ODE Tutor ready! (type 'exit' to quit)

You âœ Solve dy/dx = xy, y(0) = 1

Tutor âœ I'll solve this separable differential equation step by step.

Given: $\frac{dy}{dx} = xy$ with initial condition $y(0) = 1$

**Step 1: Separate variables**
$\frac{dy}{y} = x \, dx$

**Step 2: Integrate both sides**
$\int \frac{dy}{y} = \int x \, dx$
$\ln|y| = \frac{x^2}{2} + C$

**Step 3: Solve for y**
$y = Ae^{\frac{x^2}{2}}$ where $A = e^C$

**Step 4: Apply initial condition**
$y(0) = 1 = Ae^0 = A$
Therefore $A = 1$

**Final Solution:**
$$y = e^{\frac{x^2}{2}}$$

**Verification:** $\frac{dy}{dx} = xe^{\frac{x^2}{2}} = xy$ âœ“
```

### REST API Mode
```powershell
scripts\run_api.ps1
```

The API will be available at `http://localhost:8000`

**Endpoints:**
- `GET /` - API status
- `GET /health` - Health check  
- `POST /generate` - Generate ODE solution

**Example API usage:**
```bash
curl -X POST http://localhost:8000/generate \
  -H "Content-Type: application/json" \
  -d '{"question": "Solve y'' + 4y = 0"}'
```

---

## ğŸ› ï¸ **Customization**

| What to change | Where to edit |
|----------------|---------------|
| Model (e.g., use 7B variant) | `config/settings.yaml` â†’ `model.local_path` |
| Generation parameters | `config/settings.yaml` â†’ `generation.*` |
| System prompt | `config/settings.yaml` â†’ `prompts.system` |
| Enable quantization | `src/chatbot.py` â†’ add `load_in_4bit=True` |
| API port | `scripts/run_api.ps1` â†’ change `--port 8000` |

---

## ğŸ” **Troubleshooting**

### Common Issues

**âŒ "CUDA out of memory"**
```yaml
# In config/settings.yaml, reduce batch size:
generation:
  max_new_tokens: 256  # Reduce from 512
```

**âŒ "conda: command not found"**
- Install Miniconda and restart PowerShell
- Or use pip: `pip install -r requirements.txt`

**âŒ "Model not found"**
```powershell
# Re-download model weights:
python download_weights.py
```

**âŒ "Import errors"**
```powershell
# Recreate environment:
conda env remove -n deepseek-ode
scripts\setup.ps1
```

**âŒ API won't start**
- Check if port 8000 is in use: `netstat -an | findstr :8000`
- Use different port: Edit `scripts\run_api.ps1`

### Performance Optimization

**For slower GPUs (< 8GB VRAM):**
```python
# In src/chatbot.py, enable quantization:
self.model = AutoModelForCausalLM.from_pretrained(
    self.model_path,
    torch_dtype="auto", 
    device_map="auto",
    load_in_4bit=True,  # Add this line
    trust_remote_code=True,
)
```

**For CPU-only systems:**
```python
# In src/chatbot.py, force CPU:
self.model = AutoModelForCausalLM.from_pretrained(
    self.model_path,
    torch_dtype="float32",
    device_map="cpu",  # Change this
    trust_remote_code=True,
)
```

---

## ğŸ“Š **System Requirements Met**

âœ… **Windows GPU server ready**  
âœ… **One-click setup with PowerShell**  
âœ… **GPU acceleration with CUDA**  
âœ… **Production-grade error handling**  
âœ… **Both CLI and API interfaces**  
âœ… **Configurable and extensible**  

https://huggingface.co/deepseek-ai/DeepSeek-R1-0528
